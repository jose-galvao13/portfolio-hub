---
import CaseStudyLayout from '../layouts/CaseStudyLayout.astro';

const metrics = [
  { icon: '‚ö°', value: '70%', label: 'Query Time Reduction' },
  { icon: 'üìä', value: '1M+', label: 'Transactions Processed' },
  { icon: 'üéØ', value: '100%', label: 'Data Integrity' },
  { icon: 'üìà', value: '5', label: 'Data Sources Integrated' }
];
---

<CaseStudyLayout
  title="Financial Data Warehouse (ETL Pipeline)"
  description="Enterprise-grade data warehouse transforming fragmented ERP data into a unified Star Schema for single-source-of-truth reporting."
  projectType="Data Engineering"
  stack={['SQL', 'Python', 'Power BI', 'Star Schema', 'ETL']}
  metrics={metrics}
  githubUrl="https://github.com/jose-galvao13/financial-warehouse"
>
  <section>
    <h2>üéØ Business Challenge</h2>
    <p>
      The finance team was drowning in <strong>data chaos</strong>: 5 different systems (ERP, CRM, Excel, legacy databases) with no unified view, leading to:
    </p>
    <ul>
      <li><strong>Conflicting Numbers:</strong> Sales report from CRM showed ‚Ç¨2.5M revenue, accounting system showed ‚Ç¨2.3M‚Äîwhich is correct?</li>
      <li><strong>Manual Reconciliation:</strong> Analysts spending 10 hours/week joining data across systems in Excel.</li>
      <li><strong>Slow Queries:</strong> Generating month-end reports took 45 minutes due to complex joins across denormalized tables.</li>
      <li><strong>No Historical Tracking:</strong> Couldn't analyze trends (e.g., "How has gross margin changed over 3 years?").</li>
    </ul>
    <blockquote>
      "We had 5 versions of 'the truth'. The CFO would ask a question and get 3 different answers depending on who responded."
    </blockquote>
  </section>

  <section>
    <h2>üí° Solution Architecture</h2>
    <p>
      I designed and implemented a <strong>centralized data warehouse</strong> using Star Schema methodology, with automated ETL pipelines ensuring data freshness and consistency.
    </p>
    
    <h3>Phase 1: Data Warehouse Design (Star Schema)</h3>
    <ul>
      <li><strong>Fact Tables:</strong> Sales Transactions, GL Entries, Inventory Movements (grain: 1 row per transaction).</li>
      <li><strong>Dimension Tables:</strong> Customers, Products, Time (Date), Accounts (Chart of Accounts), Departments.</li>
      <li><strong>Slowly Changing Dimensions (SCD Type 2):</strong> Track historical changes (e.g., customer address changes over time).</li>
    </ul>

    <h3>Phase 2: ETL Pipeline Development</h3>
    <ul>
      <li><strong>Extract:</strong> Pull data from ERP (SAP), CRM (Salesforce), Excel files (FTP), legacy SQL Server.</li>
      <li><strong>Transform:</strong> Standardize formats, handle nulls, deduplicate, calculate derived metrics (gross margin, YoY growth).</li>
      <li><strong>Load:</strong> Insert into warehouse (PostgreSQL) using UPSERT logic (update existing, insert new).</li>
      <li><strong>Scheduling:</strong> Automated nightly runs via Python + cron (incremental loads for performance).</li>
    </ul>

    <h3>Phase 3: Power BI Integration</h3>
    <ul>
      <li>Connected Power BI to warehouse via DirectQuery (real-time dashboards).</li>
      <li>Pre-built star schema enabled drag-and-drop reporting (no complex SQL needed).</li>
      <li>Created reusable DAX measures: YTD Revenue, MTD Expenses, Customer Lifetime Value.</li>
    </ul>

    <h3>Technical Implementation</h3>
    <pre><code>import pandas as pd
import psycopg2
from sqlalchemy import create_engine

# 1. Extract from multiple sources
erp_data = pd.read_sql("SELECT * FROM sales", erp_connection)
crm_data = pd.read_csv("crm_export.csv")

# 2. Transform
merged = pd.merge(erp_data, crm_data, on='customer_id', how='left')
merged['gross_margin'] = (merged['revenue'] - merged['cogs']) / merged['revenue']
merged['transaction_date'] = pd.to_datetime(merged['transaction_date'])

# 3. Load to Warehouse
warehouse_engine = create_engine('postgresql://user:pass@localhost/warehouse')

# UPSERT logic (update if exists, insert if new)
merged.to_sql('fact_sales', warehouse_engine, if_exists='append', index=False)

# 4. Update Dimensions (SCD Type 2)
existing_customers = pd.read_sql("SELECT * FROM dim_customer", warehouse_engine)
new_customers = merged[~merged['customer_id'].isin(existing_customers['customer_id'])]
new_customers.to_sql('dim_customer', warehouse_engine, if_exists='append')</code></pre>
  </section>

  <section>
    <h2>üìà Key Results & Business Impact</h2>
    
    <h3>Quantified Outcomes</h3>
    <ul>
      <li><strong>70% Faster Queries:</strong> Month-end reports down from 45min to 13min (optimized Star Schema).</li>
      <li><strong>1M+ Transactions:</strong> Warehouse handles 1.2M transactions with sub-second query response.</li>
      <li><strong>100% Data Integrity:</strong> Eliminated discrepancies through single source of truth.</li>
      <li><strong>5 Sources Unified:</strong> ERP, CRM, Excel, legacy SQL Server, bank feeds‚Äîall in one place.</li>
    </ul>

    <h3>Business Metrics Improved</h3>
    <ul>
      <li><strong>Reconciliation Time:</strong> 10 hours/week ‚Üí 1 hour/week (90% reduction).</li>
      <li><strong>Report Accuracy:</strong> Zero "version conflicts" in executive dashboards.</li>
      <li><strong>Self-Service Analytics:</strong> Business users can now build their own reports without IT help.</li>
    </ul>

    <h3>Strategic Insights</h3>
    <ul>
      <li><strong>Historical Trend Analysis:</strong> Discovered 15% decline in gross margin over 2 years due to supplier price increases.</li>
      <li><strong>Customer Segmentation:</strong> Identified top 20% of customers generating 80% of revenue.</li>
      <li><strong>Inventory Optimization:</strong> Detected slow-moving SKUs tying up ‚Ç¨500k in working capital.</li>
    </ul>
  </section>

  <section>
    <h2>üõ†Ô∏è Technical Methodology</h2>
    
    <h3>Star Schema Design Principles</h3>
    <ol>
      <li><strong>Fact Tables:</strong> Contain metrics (revenue, cost, quantity) + foreign keys to dimensions.</li>
      <li><strong>Dimension Tables:</strong> Contain descriptive attributes (customer name, product category, date).</li>
      <li><strong>Grain:</strong> Define granularity (e.g., 1 row per invoice line item).</li>
      <li><strong>Denormalization:</strong> Dimensions are denormalized for query performance.</li>
    </ol>

    <h3>ETL Best Practices</h3>
    <ul>
      <li><strong>Incremental Loads:</strong> Only load new/changed records (not full refresh every night).</li>
      <li><strong>Data Validation:</strong> Check for nulls, duplicates, referential integrity before loading.</li>
      <li><strong>Error Handling:</strong> Log failed records to error table for manual review.</li>
      <li><strong>Idempotency:</strong> ETL can be re-run without creating duplicates (UPSERT logic).</li>
    </ul>

    <h3>Performance Optimization</h3>
    <ul>
      <li><strong>Indexes:</strong> B-tree indexes on foreign keys, composite indexes on frequently filtered columns.</li>
      <li><strong>Partitioning:</strong> Fact tables partitioned by month for faster queries on recent data.</li>
      <li><strong>Materialized Views:</strong> Pre-aggregated tables for common reports (daily/monthly totals).</li>
    </ul>
  </section>

  <section>
    <h2>üéì Lessons Learned</h2>
    
    <h3>What Worked</h3>
    <ul>
      <li><strong>Stakeholder Alignment:</strong> Involved end-users early to define business requirements and key metrics.</li>
      <li><strong>Incremental Development:</strong> Started with Sales data, then added Finance, then Inventory (iterative approach).</li>
      <li><strong>Documentation:</strong> Created data dictionary explaining every field (reduced support tickets by 80%).</li>
    </ul>

    <h3>Challenges Overcome</h3>
    <ul>
      <li><strong>Data Quality:</strong> Source systems had 12% duplicate records; implemented fuzzy matching to deduplicate.</li>
      <li><strong>Schema Changes:</strong> ERP vendor changed field names in update; added abstraction layer to handle schema evolution.</li>
      <li><strong>Performance Degradation:</strong> Queries slowed as data grew; added partitioning and archived old data.</li>
    </ul>
  </section>

  <section>
    <h2>üöÄ Future Enhancements</h2>
    <ul>
      <li><strong>Real-Time Streaming:</strong> Move from batch (nightly) to streaming (Apache Kafka) for real-time dashboards.</li>
      <li><strong>Data Quality Framework:</strong> Automated data profiling and anomaly detection (Great Expectations).</li>
      <li><strong>Cloud Migration:</strong> Migrate from on-prem PostgreSQL to cloud (Snowflake/BigQuery).</li>
      <li><strong>Machine Learning:</strong> Add predictive models (sales forecasting, churn prediction) directly in warehouse.</li>
    </ul>
  </section>

  <section>
    <h2>üìö Technical Stack Deep Dive</h2>
    <ul>
      <li><strong>PostgreSQL:</strong> OLAP database optimized for analytical queries (Star Schema).</li>
      <li><strong>Python (Pandas, SQLAlchemy):</strong> ETL scripting, data transformations.</li>
      <li><strong>Power BI:</strong> Visualization layer, DAX measures, self-service reporting.</li>
      <li><strong>Cron:</strong> Job scheduling for nightly ETL runs.</li>
      <li><strong>Git:</strong> Version control for ETL scripts and schema migrations.</li>
    </ul>
  </section>
</CaseStudyLayout>
